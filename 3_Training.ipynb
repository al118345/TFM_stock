{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 0. Libraries"],"metadata":{"id":"JlB9yUh_pLqi"}},{"cell_type":"code","source":["!pip install stable_baselines3==2.3.2\n","!pip install gymnasium==0.29.1\n","!pip install shimmy>=0.2.1"],"metadata":{"id":"ox_JnWTDBQsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install --upgrade stable-baselines3 gym gymnasium"],"metadata":{"id":"4gGPGQeIvT2Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"gUVqt0MhChla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/My Drive/Codis_TFM/')\n","import os\n","#import gym\n","import numpy as np\n","import pandas as pd\n","import time\n","import matplotlib.pyplot as plt\n","\n","from stable_baselines3 import PPO, A2C, DDPG\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n","from trading_environment import StockTradingEnv"],"metadata":{"id":"OVNhlbwdAnd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"],"metadata":{"id":"1_BIOneG3uzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Reading data"],"metadata":{"id":"pRaU01gkpQ81"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/My Drive/Codis_TFM/data/preprocessed_data_1d.csv')\n","print(data.shape)\n","print(data.columns)\n","data.head()"],"metadata":{"id":"tt4tnEleGqqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start_date = '2014-01-01'\n","end_date = '2023-12-31'"],"metadata":{"id":"cSpZ3upy5561"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Functions training"],"metadata":{"id":"qhtFIGRjpbj6"}},{"cell_type":"code","source":["def data_filtering(df,start,end):\n","\n","    df = df.reset_index(drop=True)\n","    df[\"datadate\"] = pd.to_datetime(df[\"datadate\"])\n","    data = df[(df.datadate >= start) & (df.datadate < end)]\n","    data=data.sort_values(['datadate','tic'],ignore_index=True)\n","    data.index = data.datadate.factorize()[0]\n","    return data\n","\n","def create_timeline(start_date, end_date, initial_training_months):\n","    date_range = pd.date_range(start=start_date, end=end_date, freq=\"ME\")\n","\n","    # Initialize lists to store the periods\n","    training_periods = []\n","    validation_periods = []\n","    trading_periods = []\n","\n","    # Iterate through quarters starting from the end of the initial training window\n","    for i in range(initial_training_months, len(date_range) - 6, 3):  # Ensure 3 months for validation and trading remain\n","        # Define training period\n","        training_start = pd.Timestamp(\"2014-01-01\")  # Always starts on 1st January 2014\n","        training_end = date_range[i - 1]  # Up to the last month before validation\n","        training_periods.append((training_start, training_end))\n","\n","        # Define validation period (next 3 months)\n","        validation_start = date_range[i].replace(day=1)\n","        validation_end = date_range[i + 2].replace(day=date_range[i + 2].days_in_month)\n","        validation_periods.append((validation_start, validation_end))\n","\n","        # Define trading period (next 3 months after validation)\n","        trading_start = date_range[i + 3].replace(day=1)\n","        trading_end = date_range[i + 5].replace(day=date_range[i + 5].days_in_month)\n","        trading_periods.append((trading_start, trading_end))\n","\n","    # Handle the last period if it exists\n","    if len(date_range) >= initial_training_months + 6:\n","        last_training_end = date_range[-7]  # Correct Training End for the last period\n","        if last_training_end < date_range[-1]:\n","            validation_start = date_range[-6].replace(day=1)\n","            validation_end = date_range[-4].replace(day=date_range[-4].days_in_month)\n","            trading_start = date_range[-3].replace(day=1)\n","            trading_end = date_range[-1].replace(day=date_range[-1].days_in_month)\n","\n","            training_periods.append((pd.Timestamp(\"2014-01-01\"), last_training_end))\n","            validation_periods.append((validation_start, validation_end))\n","            trading_periods.append((trading_start, trading_end))\n","\n","    # Create a DataFrame to organize the results\n","    timeline_df = pd.DataFrame({\n","        \"Training_Start\": [p[0] for p in training_periods],\n","        \"Training_End\": [p[1] for p in training_periods],\n","        \"Validation_Start\": [p[0] for p in validation_periods],\n","        \"Validation_End\": [p[1] for p in validation_periods],\n","        \"Trading_Start\": [p[0] for p in trading_periods],\n","        \"Trading_End\": [p[1] for p in trading_periods],\n","    })\n","\n","    return timeline_df"],"metadata":{"id":"icZgLONfbI2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configurar el entorno\n","def make_env(df, stock_dim, state_space, action_space, tech_indicators, macro_variables, initial_amount=1000000):\n","    return DummyVecEnv([lambda: StockTradingEnv(\n","        df=df,\n","        stock_dim=stock_dim,\n","        state_space=state_space,\n","        action_space=action_space,\n","        tech_indicator_list=tech_indicators,\n","        macro_variable_list=macro_variables,\n","        initial_amount=initial_amount\n","    )])\n","\n","# Función para entrenar A2C\n","def train_load_a2c(env_train, model_name, timesteps=25000):\n","\n","    model_path = os.path.join(MODEL_DIR, f\"{model_name}.zip\")\n","    csv_path = os.path.join(MODEL_DIR, f\"{model_name}_asset_memory.csv\")\n","\n","    if os.path.exists(model_path):\n","        print(f\"[INFO] Cargando modelo existente: {model_path}\")\n","        model = A2C.load(model_path, env=env_train)\n","        df_asset_memory = pd.read_csv(csv_path)\n","    else:\n","        print(f\"[INFO] Entrenando nuevo modelo: {model_name} ({start_date} a {end_date})\")\n","        model = A2C(\n","            \"MlpPolicy\",\n","            env_train,\n","            learning_rate=0.0007,\n","            n_steps=20,\n","            gamma=0.99,\n","            verbose=0\n","        )\n","        model.learn(total_timesteps=timesteps)\n","        model.save(model_path)\n","\n","        original_env = env_train.envs[0]\n","        df_asset_memory = pd.DataFrame(original_env.asset_memory, columns=[\"Portfolio Value\"])\n","        df_asset_memory.to_csv(csv_path, index=False)\n","    return model, df_asset_memory\n","\n","# Función para entrenar PPO\n","def train_load_ppo(env_train, model_name, timesteps=50000):\n","\n","    model_path = os.path.join(MODEL_DIR, f\"{model_name}.zip\")\n","    csv_path = os.path.join(MODEL_DIR, f\"{model_name}_asset_memory.csv\")\n","\n","    if os.path.exists(model_path):\n","        print(f\"[INFO] Cargando modelo existente: {model_path}\")\n","        model = PPO.load(model_path, env=env_train)\n","        df_asset_memory = pd.read_csv(csv_path)\n","    else:\n","        print(f\"[INFO] Entrenando nuevo modelo: {model_name} ({start_date} a {end_date})\")\n","        model = PPO(\n","            \"MlpPolicy\",\n","            env_train,\n","            learning_rate=0.0003,\n","            ent_coef=0.01,\n","            n_steps=128,\n","            clip_range=0.2,\n","            verbose=0\n","        )\n","        model.learn(total_timesteps=timesteps)\n","        model.save(model_path)\n","\n","        original_env = env_train.envs[0]\n","        df_asset_memory = pd.DataFrame(original_env.asset_memory, columns=[\"Portfolio Value\"])\n","        df_asset_memory.to_csv(csv_path, index=False)\n","    return model, df_asset_memory\n","\n","# Función para entrenar DDPG\n","def train_load_ddpg(env_train, model_name, timesteps=10000):\n","\n","    model_path = os.path.join(MODEL_DIR, f\"{model_name}.zip\")\n","    csv_path = os.path.join(MODEL_DIR, f\"{model_name}_asset_memory.csv\")\n","\n","    if os.path.exists(model_path):\n","        print(f\"[INFO] Cargando modelo existente: {model_path}\")\n","        model = DDPG.load(model_path, env=env_train)\n","        df_asset_memory = pd.read_csv(csv_path)\n","    else:\n","        print(f\"[INFO] Entrenando nuevo modelo: {model_name} ({start_date} a {end_date})\")\n","        n_actions = env_train.action_space.shape[-1]\n","        action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n","        model = DDPG(\n","            \"MlpPolicy\",\n","            env_train,\n","            learning_rate=0.001,\n","            batch_size=64,\n","            action_noise=action_noise,\n","            verbose=0\n","        )\n","        model.learn(total_timesteps=timesteps)\n","        model.save(model_path)\n","\n","        original_env = env_train.envs[0]\n","        df_asset_memory = pd.DataFrame(original_env.asset_memory, columns=[\"Portfolio Value\"])\n","        df_asset_memory.to_csv(csv_path, index=False)\n","    return model, df_asset_memory\n","\n","# Función para evaluar mejor modelo por sharpe ratio\n","def evaluate_model(model, env):\n","    obs = env.reset()\n","    total_rewards = []\n","\n","    for _ in range(len(env.get_attr('df')[0].index.unique())):\n","        action, _states = model.predict(obs)\n","        obs, rewards, done, info = env.step(action)\n","\n","        # Asegurarse de que 'rewards' es un número escalar\n","        if isinstance(rewards, np.ndarray):\n","            rewards = rewards.item()  # Convierte el array a un escalar\n","\n","        total_rewards.append(rewards)\n","\n","        if done:\n","            break\n","\n","    # Calcular Sharpe Ratio\n","    returns = pd.Series(total_rewards)\n","    sharpe_ratio = returns.mean() / returns.std() * (252**0.5)\n","    return sharpe_ratio"],"metadata":{"id":"BCcBNNHSxdBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_with_model(model, model_name, env):\n","    obs = env.reset()\n","    df_asset_memory = []\n","\n","    for _ in range(len(env.envs[0].df.index.unique())):\n","        action, _ = model.predict(obs)\n","        obs, rewards, done, info = env.step(action)\n","\n","        # Guardar información del valor del portafolio\n","        portfolio_value = env.envs[0].asset_memory[-1] if hasattr(env.envs[0], 'asset_memory') else 0\n","        df_asset_memory.append(portfolio_value)\n","\n","        if done:\n","            break\n","\n","    # Convertir a DataFrame\n","    df_asset_memory = pd.DataFrame(df_asset_memory, columns=[f\"Portfolio_value_{model_name}\"])\n","    return df_asset_memory"],"metadata":{"id":"6pD_khNtMiHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función para ejecutar la estrategia de ensamble\n","def ensemble_strategy(MODEL_DIR, timeline_df, df, stock_dim, state_space, action_space, tech_indicators, macro_variables):\n","    print(\"[INFO] Iniciando estrategia de ensamble con hiperparámetros generales...\")\n","\n","    # DataFrames acumulativos para almacenar las predicciones\n","    test_data = []\n","    a2c_data = []\n","    ppo_data = []\n","    ddpg_data = []\n","\n","    for index, row in timeline_df.iterrows():\n","        # Extraer períodos\n","        start_train = row[\"Training_Start\"].strftime(\"%Y-%m-%d\")\n","        end_train = row[\"Training_End\"].strftime(\"%Y-%m-%d\")\n","        start_val = row[\"Validation_Start\"].strftime(\"%Y-%m-%d\")\n","        end_val = row[\"Validation_End\"].strftime(\"%Y-%m-%d\")\n","        start_trade = row[\"Trading_Start\"].strftime(\"%Y-%m-%d\")\n","        end_trade = row[\"Trading_End\"].strftime(\"%Y-%m-%d\")\n","\n","        print(f\"[INFO] Procesando período {start_train} a {end_val}...\")\n","\n","        # Filtrar datos para entrenamiento y validación\n","        df_train = data_filtering(df, start_train, end_train)\n","        df_val = data_filtering(df, start_val, end_val)\n","        df_trade = data_filtering(df, start_trade, end_trade)\n","\n","        # Crear entornos\n","        print('Creación de entornos')\n","        env_train = make_env(df_train, stock_dim, state_space, action_space, tech_indicators, macro_variables)\n","        env_val = make_env(df_val, stock_dim, state_space, action_space, tech_indicators, macro_variables)\n","\n","        if len(test_data) > 0:\n","            print('dentro del if')\n","            print(previous_final_value)\n","            previous_final_value = test_data[-1][\"Portfolio_value_ensemble\"].iloc[-1]\n","        else:\n","            previous_final_value = 1000000  # Valor inicial por defecto\n","\n","        # Crear entorno con capital inicial ajustado\n","        env_trade = make_env(\n","            df_trade, stock_dim, state_space, action_space, tech_indicators, macro_variables,\n","            initial_amount=previous_final_value\n","        )\n","\n","        # Entrenar modelos\n","        print(\"[INFO] Entrenando A2C...\")\n","        a2c_model, _ = train_load_a2c(env_train, f\"A2C_{start_train}_{end_train}\")\n","\n","        print(\"[INFO] Entrenando PPO...\")\n","        ppo_model, _ = train_load_ppo(env_train, f\"PPO_{start_train}_{end_train}\")\n","\n","        print(\"[INFO] Entrenando DDPG...\")\n","        ddpg_model, _ = train_load_ddpg(env_train, f\"DDPG_{start_train}_{end_train}\")\n","\n","        # Evaluar modelos\n","        print(\"[INFO] Evaluando modelos...\")\n","        a2c_sharpe = evaluate_model(a2c_model, env_val)\n","        ppo_sharpe = evaluate_model(ppo_model, env_val)\n","        ddpg_sharpe = evaluate_model(ddpg_model, env_val)\n","\n","        # Seleccionar el mejor modelo\n","        sharpe_ratios = {\"A2C\": a2c_sharpe, \"PPO\": ppo_sharpe, \"DDPG\": ddpg_sharpe}\n","        best_model_name = max(sharpe_ratios, key=sharpe_ratios.get)\n","        best_model = {\"A2C\": a2c_model, \"PPO\": ppo_model, \"DDPG\": ddpg_model}[best_model_name]\n","        print(f\"[INFO] Mejor modelo para período {start_train} a {end_val}: {best_model_name} con Sharpe Ratio {sharpe_ratios[best_model_name]:.2f}\")\n","\n","        # Predicción con cada modelo\n","        print(f\"[INFO] Predicción con modelos individuales para período {start_trade} a {end_trade}...\")\n","        for model_name, model, data_accumulator in zip(\n","            [\"A2C\", \"PPO\", \"DDPG\"],\n","            [a2c_model, ppo_model, ddpg_model],\n","            [a2c_data, ppo_data, ddpg_data]\n","        ):\n","            df_predicted_asset_memory = predict_with_model(model, model_name, env_trade)\n","            df_predicted_asset_memory = df_predicted_asset_memory.iloc[:-1]\n","\n","            # Agregar al acumulador correspondiente\n","            data_accumulator.append(df_predicted_asset_memory)\n","\n","        # Predicción con el modelo seleccionado\n","        print(f\"[INFO] Predicción con el mejor modelo ({best_model_name}) para período {start_trade} a {end_trade}...\")\n","        df_predicted_asset_memory = predict_with_model(best_model, 'ensemble', env_trade)\n","\n","        df_predicted_asset_memory = df_predicted_asset_memory.iloc[:-1]\n","        test_data.append(df_predicted_asset_memory)\n","\n","        print(f\"[INFO] Completado período {start_train} a {end_trade}.\")\n","        print(\"===============================================\")\n","\n","    # Guardar CSV acumulativos\n","    test_data = pd.concat(test_data, ignore_index=True)\n","    a2c_data = pd.concat(a2c_data, ignore_index=True)\n","    ppo_data = pd.concat(ppo_data, ignore_index=True)\n","    ddpg_data = pd.concat(ddpg_data, ignore_index=True)\n","\n","    summary_test_data = pd.concat([test_data, a2c_data, ppo_data, ddpg_data], axis=1)\n","    summary_test_data.to_csv(f\"{MODEL_DIR}/summary_test_data.csv\", index=False)\n","\n","    print(\"[INFO] CSV combinados guardados con éxito.\")"],"metadata":{"id":"vehljTVE8oFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##################################################################################\n","timeline_df = create_timeline(start_date, end_date, 60)\n","\n","# Configuración de rutas para guardar los modelos\n","MODEL_DIR = '/content/drive/My Drive/Codis_TFM/saved_models_v1'\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","# Configuración del entorno\n","stock_dim = 10\n","tech_indicator = ['macd', 'rsi', 'bandwidth', 'ichimoku', 'stoch_k', 'roc', 'vr', 'atr_14']\n","macro_variable = ['GDP_growth_developed', 'GDP_growth_emerging', 'GDP_growth_us', 'inflation_developed', 'inflation_emerging', 'inflation_us']\n","state_space = 1 + stock_dim * (1 + 1 + len(tech_indicator)) + len(macro_variable)\n","action_space = stock_dim\n","\n","init_time = time.time()\n","# Ejecutar la estrategia con la línea de tiempo\n","ensemble_strategy(MODEL_DIR, timeline_df, data, stock_dim, state_space, action_space, tech_indicator, macro_variable)\n","finish_time = time.time()\n","print(f\"Tiempo ejecución{(finish_time - init_time)/60} en minutos.\")"],"metadata":{"id":"QFTys0TR4RKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-fZnQiPdYYg2"},"execution_count":null,"outputs":[]}]}